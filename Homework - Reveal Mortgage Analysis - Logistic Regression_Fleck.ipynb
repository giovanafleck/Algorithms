{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lending disparities using Logistic Regression\n",
    "\n",
    "**The story:** https://www.revealnews.org/article/for-people-of-color-banks-are-shutting-the-door-to-homeownership/\n",
    "\n",
    "**Author:** Aaron Glantz and Emmanuel Martinez\n",
    "\n",
    "**Topics:** Logistic regression, odds ratios\n",
    "\n",
    "**Datasets**\n",
    "\n",
    "* **philadelphia-mortgages.csv:** Philadelphia mortgage data for 2015\n",
    "    - A subset of HMDA LAR data from [FFEIC](https://www.ffiec.gov/hmda/hmdaproducts.htm)\n",
    "    - Codebook is `2015HMDACodeSheet.pdf`\n",
    "    - A [guide to HMDA reporting](https://www.ffiec.gov/hmda/guide.htm)\n",
    "    - I've massaged it slightly to make processing a bit easier\n",
    "* **nhgis0006_ds233_20175_2017_tract.csv:**\n",
    "    - Table B03002: Hispanic or Latino Origin by Race\n",
    "    - 2013-2017 American Community Survey data US Census Bureau, from [NHGIS](https://data2.nhgis.org/main)\n",
    "    - Codebook is `nhgis0006_ds233_20175_2017_tract_codebook.txt`\n",
    "* **lending_disparities_whitepaper_180214.pdf:** the whitepaper outlining Reveal's methodology\n",
    "\n",
    "## What's the goal?\n",
    "\n",
    "Do banks provide mortgages at disparate rates between white applicants and people of color? We're going to look at the following variables to find out:\n",
    "\n",
    "* Race/Ethnicity\n",
    "    - Native American\n",
    "    - Asian\n",
    "    - Black\n",
    "    - Native Hawaiian\n",
    "    - Hispanic/Latino\n",
    "    - Race and ethnicity were not reported\n",
    "* Sex\n",
    "* Whether there was a co-applicant\n",
    "* Applicant’s annual income (includes co-applicant income)\n",
    "* Loan amount\n",
    "* Ratio between the loan amount and the applicant’s income\n",
    "* Ratio between the median income of the census tract and the median income of the metro area\n",
    "* Racial and ethnic breakdown by percentage for each census tract\n",
    "* Regulating agency of the lending institution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Import pandas as usual, but also import numpy. We'll need it for logarithms and exponents.\n",
    "\n",
    "Some of our datasets have a lot of columns, so you'll also want to use `pd.set_option` to display up to 100 columns or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_colwidth\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is each row of our data?\n",
    "\n",
    "If you aren't sure, you might need to look at either the whitepaper or the codebook. You'll need to look at them both eventually, so might as well get started now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context Fields \n",
    "        GISJOIN:     GIS Join Match Code\n",
    "        YEAR:        Data File Year\n",
    "        REGIONA:     Region Code\n",
    "        DIVISIONA:   Division Code\n",
    "        STATE:       State Name\n",
    "        STATEA:      State Code\n",
    "        COUNTY:      County Name\n",
    "        COUNTYA:     County Code\n",
    "        COUSUBA:     County Subdivision Code\n",
    "        PLACEA:      Place Code\n",
    "        TRACTA:      Census Tract Code\n",
    "        BLKGRPA:     Block Group Code\n",
    "        CONCITA:     Consolidated City Code\n",
    "        AIANHHA:     American Indian Area/Alaska Native Area/Hawaiian Home Land Code\n",
    "        RES_ONLYA:   American Indian Area/Alaska Native Area (Reservation or Statistical Entity Only) Code\n",
    "        TRUSTA:      American Indian Area (Off-Reservation Trust Land Only)/Hawaiian Home Land Code\n",
    "        AITSCEA:     Tribal Subdivision/Remainder Code\n",
    "        ANRCA:       Alaska Native Regional Corporation Code\n",
    "        CBSAA:       Metropolitan Statistical Area/Micropolitan Statistical Area Code\n",
    "        CSAA:        Combined Statistical Area Code\n",
    "        METDIVA:     Metropolitan Division Code\n",
    "        NECTAA:      New England City and Town Area Code\n",
    "        CNECTAA:     Combined New England City and Town Area Code\n",
    "        NECTADIVA:   New England City and Town Area Division Code\n",
    "        UAA:         Urban Area Code\n",
    "        CDCURRA:     Congressional District (2013-2017, 113th-114th Congress) Code\n",
    "        SLDUA:       State Legislative District (Upper Chamber) Code\n",
    "        SLDLA:       State Legislative District (Lower Chamber) Code\n",
    "        ZCTA5A:      5-Digit ZIP Code Tabulation Area Code\n",
    "        SUBMCDA:     Subminor Civil Division Code\n",
    "        SDELMA:      School District (Elementary)/Remainder Code\n",
    "        SDSECA:      School District (Secondary)/Remainder Code\n",
    "        SDUNIA:      School District (Unified)/Remainder Code\n",
    "        PUMA5A:      Public Use Microdata Sample Area (PUMA) Code\n",
    "        BTTRA:       Tribal Census Tract Code\n",
    "        BTBGA:       Tribal Block Group Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in your data\n",
    "\n",
    "Read in our Philadelphia mortgage data and take a peek at the first few rows.\n",
    "\n",
    "* **Tip:** As always, census tract columns like to cause problems if they're read in as numbers. Make sure pandas reads it in as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>census_tract</th>\n",
       "      <th>county_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>applicant_sex</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>property_type</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>action_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>agency_code</th>\n",
       "      <th>tract_to_msa_income_percent</th>\n",
       "      <th>applicant_race</th>\n",
       "      <th>co_applicant_sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010100</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>97.09</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>026400</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>98.27</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>028100</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>72.28</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>015800</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>105.87</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>035800</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>139.62</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  census_tract county_code state_code applicant_sex income loan_amount  \\\n",
       "0       010100         101         42             3     26           5   \n",
       "1       026400         101         42             2     26          40   \n",
       "2       028100         101         42             2     22          20   \n",
       "3       015800         101         42             2     57          36   \n",
       "4       035800         101         42             1     80          34   \n",
       "\n",
       "  loan_type property_type occupancy action_type loan_purpose agency_code  \\\n",
       "0         1             1         1           4            2           5   \n",
       "1         1             1         1           4            2           5   \n",
       "2         1             1         1           5            2           5   \n",
       "3         1             1         1           5            3           5   \n",
       "4         1             1         1           1            3           5   \n",
       "\n",
       "  tract_to_msa_income_percent applicant_race co_applicant_sex  \n",
       "0                       97.09              6                5  \n",
       "1                       98.27              3                5  \n",
       "2                       72.28              6                5  \n",
       "3                      105.87              6                5  \n",
       "4                      139.62              5                2  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/philadelphia-mortgages.csv', dtype='str')\n",
    "df.census_tract = df.census_tract.str.replace('.','')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GISJOIN</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>REGIONA</th>\n",
       "      <th>DIVISIONA</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATEA</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>COUNTYA</th>\n",
       "      <th>COUSUBA</th>\n",
       "      <th>PLACEA</th>\n",
       "      <th>TRACTA</th>\n",
       "      <th>BLKGRPA</th>\n",
       "      <th>CONCITA</th>\n",
       "      <th>AIANHHA</th>\n",
       "      <th>RES_ONLYA</th>\n",
       "      <th>TRUSTA</th>\n",
       "      <th>AITSCEA</th>\n",
       "      <th>ANRCA</th>\n",
       "      <th>CBSAA</th>\n",
       "      <th>CSAA</th>\n",
       "      <th>METDIVA</th>\n",
       "      <th>NECTAA</th>\n",
       "      <th>CNECTAA</th>\n",
       "      <th>NECTADIVA</th>\n",
       "      <th>UAA</th>\n",
       "      <th>CDCURRA</th>\n",
       "      <th>SLDUA</th>\n",
       "      <th>SLDLA</th>\n",
       "      <th>ZCTA5A</th>\n",
       "      <th>SUBMCDA</th>\n",
       "      <th>SDELMA</th>\n",
       "      <th>SDSECA</th>\n",
       "      <th>SDUNIA</th>\n",
       "      <th>PUMA5A</th>\n",
       "      <th>BTTRA</th>\n",
       "      <th>BTBGA</th>\n",
       "      <th>NAME_E</th>\n",
       "      <th>ADK5E001</th>\n",
       "      <th>ADK5E002</th>\n",
       "      <th>ADK5E003</th>\n",
       "      <th>ADK5E004</th>\n",
       "      <th>ADK5E005</th>\n",
       "      <th>ADK5E006</th>\n",
       "      <th>ADK5E007</th>\n",
       "      <th>ADK5E008</th>\n",
       "      <th>ADK5E009</th>\n",
       "      <th>ADK5E010</th>\n",
       "      <th>ADK5E011</th>\n",
       "      <th>ADK5E012</th>\n",
       "      <th>ADK5E013</th>\n",
       "      <th>ADK5E014</th>\n",
       "      <th>ADK5E015</th>\n",
       "      <th>ADK5E016</th>\n",
       "      <th>ADK5E017</th>\n",
       "      <th>ADK5E018</th>\n",
       "      <th>ADK5E019</th>\n",
       "      <th>ADK5E020</th>\n",
       "      <th>ADK5E021</th>\n",
       "      <th>NAME_M</th>\n",
       "      <th>ADK5M001</th>\n",
       "      <th>ADK5M002</th>\n",
       "      <th>ADK5M003</th>\n",
       "      <th>ADK5M004</th>\n",
       "      <th>ADK5M005</th>\n",
       "      <th>ADK5M006</th>\n",
       "      <th>ADK5M007</th>\n",
       "      <th>ADK5M008</th>\n",
       "      <th>ADK5M009</th>\n",
       "      <th>ADK5M010</th>\n",
       "      <th>ADK5M011</th>\n",
       "      <th>ADK5M012</th>\n",
       "      <th>ADK5M013</th>\n",
       "      <th>ADK5M014</th>\n",
       "      <th>ADK5M015</th>\n",
       "      <th>ADK5M016</th>\n",
       "      <th>ADK5M017</th>\n",
       "      <th>ADK5M018</th>\n",
       "      <th>ADK5M019</th>\n",
       "      <th>ADK5M020</th>\n",
       "      <th>ADK5M021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G0100010020100</td>\n",
       "      <td>2011-2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Census Tract 201, Autauga County, Alabama</td>\n",
       "      <td>1948</td>\n",
       "      <td>1931</td>\n",
       "      <td>1703</td>\n",
       "      <td>150</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Census Tract 201, Autauga County, Alabama</td>\n",
       "      <td>203</td>\n",
       "      <td>212</td>\n",
       "      <td>229</td>\n",
       "      <td>126</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>44</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G0100010020200</td>\n",
       "      <td>2011-2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Census Tract 202, Autauga County, Alabama</td>\n",
       "      <td>2156</td>\n",
       "      <td>2139</td>\n",
       "      <td>872</td>\n",
       "      <td>1149</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Census Tract 202, Autauga County, Alabama</td>\n",
       "      <td>268</td>\n",
       "      <td>268</td>\n",
       "      <td>207</td>\n",
       "      <td>250</td>\n",
       "      <td>11</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>62</td>\n",
       "      <td>11</td>\n",
       "      <td>62</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G0100010020300</td>\n",
       "      <td>2011-2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Census Tract 203, Autauga County, Alabama</td>\n",
       "      <td>2968</td>\n",
       "      <td>2968</td>\n",
       "      <td>2212</td>\n",
       "      <td>551</td>\n",
       "      <td>15</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Census Tract 203, Autauga County, Alabama</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>372</td>\n",
       "      <td>190</td>\n",
       "      <td>22</td>\n",
       "      <td>62</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>135</td>\n",
       "      <td>11</td>\n",
       "      <td>135</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G0100010020400</td>\n",
       "      <td>2011-2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Census Tract 204, Autauga County, Alabama</td>\n",
       "      <td>4423</td>\n",
       "      <td>3959</td>\n",
       "      <td>3662</td>\n",
       "      <td>162</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>464</td>\n",
       "      <td>30</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>372</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>Census Tract 204, Autauga County, Alabama</td>\n",
       "      <td>493</td>\n",
       "      <td>353</td>\n",
       "      <td>343</td>\n",
       "      <td>133</td>\n",
       "      <td>81</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>82</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>437</td>\n",
       "      <td>33</td>\n",
       "      <td>50</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>456</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G0100010020500</td>\n",
       "      <td>2011-2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>1</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Census Tract 205, Autauga County, Alabama</td>\n",
       "      <td>10763</td>\n",
       "      <td>10683</td>\n",
       "      <td>7368</td>\n",
       "      <td>2674</td>\n",
       "      <td>0</td>\n",
       "      <td>412</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>229</td>\n",
       "      <td>49</td>\n",
       "      <td>180</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Census Tract 205, Autauga County, Alabama</td>\n",
       "      <td>624</td>\n",
       "      <td>616</td>\n",
       "      <td>796</td>\n",
       "      <td>742</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>166</td>\n",
       "      <td>74</td>\n",
       "      <td>149</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          GISJOIN       YEAR  REGIONA  DIVISIONA    STATE  STATEA  \\\n",
       "0  G0100010020100  2011-2015      NaN        NaN  Alabama       1   \n",
       "1  G0100010020200  2011-2015      NaN        NaN  Alabama       1   \n",
       "2  G0100010020300  2011-2015      NaN        NaN  Alabama       1   \n",
       "3  G0100010020400  2011-2015      NaN        NaN  Alabama       1   \n",
       "4  G0100010020500  2011-2015      NaN        NaN  Alabama       1   \n",
       "\n",
       "           COUNTY  COUNTYA  COUSUBA  PLACEA  TRACTA  BLKGRPA  CONCITA  \\\n",
       "0  Autauga County        1      NaN     NaN   20100      NaN      NaN   \n",
       "1  Autauga County        1      NaN     NaN   20200      NaN      NaN   \n",
       "2  Autauga County        1      NaN     NaN   20300      NaN      NaN   \n",
       "3  Autauga County        1      NaN     NaN   20400      NaN      NaN   \n",
       "4  Autauga County        1      NaN     NaN   20500      NaN      NaN   \n",
       "\n",
       "   AIANHHA  RES_ONLYA  TRUSTA  AITSCEA  ANRCA  CBSAA  CSAA  METDIVA  NECTAA  \\\n",
       "0      NaN        NaN     NaN      NaN    NaN    NaN   NaN      NaN     NaN   \n",
       "1      NaN        NaN     NaN      NaN    NaN    NaN   NaN      NaN     NaN   \n",
       "2      NaN        NaN     NaN      NaN    NaN    NaN   NaN      NaN     NaN   \n",
       "3      NaN        NaN     NaN      NaN    NaN    NaN   NaN      NaN     NaN   \n",
       "4      NaN        NaN     NaN      NaN    NaN    NaN   NaN      NaN     NaN   \n",
       "\n",
       "   CNECTAA  NECTADIVA  UAA  CDCURRA  SLDUA  SLDLA  ZCTA5A  SUBMCDA  SDELMA  \\\n",
       "0      NaN        NaN  NaN      NaN    NaN    NaN     NaN      NaN     NaN   \n",
       "1      NaN        NaN  NaN      NaN    NaN    NaN     NaN      NaN     NaN   \n",
       "2      NaN        NaN  NaN      NaN    NaN    NaN     NaN      NaN     NaN   \n",
       "3      NaN        NaN  NaN      NaN    NaN    NaN     NaN      NaN     NaN   \n",
       "4      NaN        NaN  NaN      NaN    NaN    NaN     NaN      NaN     NaN   \n",
       "\n",
       "   SDSECA  SDUNIA  PUMA5A  BTTRA  BTBGA  \\\n",
       "0     NaN     NaN     NaN    NaN    NaN   \n",
       "1     NaN     NaN     NaN    NaN    NaN   \n",
       "2     NaN     NaN     NaN    NaN    NaN   \n",
       "3     NaN     NaN     NaN    NaN    NaN   \n",
       "4     NaN     NaN     NaN    NaN    NaN   \n",
       "\n",
       "                                      NAME_E  ADK5E001  ADK5E002  ADK5E003  \\\n",
       "0  Census Tract 201, Autauga County, Alabama      1948      1931      1703   \n",
       "1  Census Tract 202, Autauga County, Alabama      2156      2139       872   \n",
       "2  Census Tract 203, Autauga County, Alabama      2968      2968      2212   \n",
       "3  Census Tract 204, Autauga County, Alabama      4423      3959      3662   \n",
       "4  Census Tract 205, Autauga County, Alabama     10763     10683      7368   \n",
       "\n",
       "   ADK5E004  ADK5E005  ADK5E006  ADK5E007  ADK5E008  ADK5E009  ADK5E010  \\\n",
       "0       150         6        12         0         0        60         0   \n",
       "1      1149         0        50         0         0        68         0   \n",
       "2       551        15        41         8         0       141         0   \n",
       "3       162        69         0         0        48        18         5   \n",
       "4      2674         0       412         0         0       229        49   \n",
       "\n",
       "   ADK5E011  ADK5E012  ADK5E013  ADK5E014  ADK5E015  ADK5E016  ADK5E017  \\\n",
       "0        60        17        17         0         0         0         0   \n",
       "1        68        17        14         0         0         0         0   \n",
       "2       141         0         0         0         0         0         0   \n",
       "3        13       464        30        42         0         0         0   \n",
       "4       180        80        80         0         0         0         0   \n",
       "\n",
       "   ADK5E018  ADK5E019  ADK5E020  ADK5E021  \\\n",
       "0         0         0         0         0   \n",
       "1         3         0         0         0   \n",
       "2         0         0         0         0   \n",
       "3       372        20        20         0   \n",
       "4         0         0         0         0   \n",
       "\n",
       "                                      NAME_M  ADK5M001  ADK5M002  ADK5M003  \\\n",
       "0  Census Tract 201, Autauga County, Alabama       203       212       229   \n",
       "1  Census Tract 202, Autauga County, Alabama       268       268       207   \n",
       "2  Census Tract 203, Autauga County, Alabama       404       404       372   \n",
       "3  Census Tract 204, Autauga County, Alabama       493       353       343   \n",
       "4  Census Tract 205, Autauga County, Alabama       624       616       796   \n",
       "\n",
       "   ADK5M004  ADK5M005  ADK5M006  ADK5M007  ADK5M008  ADK5M009  ADK5M010  \\\n",
       "0       126         8        16        11        11        44        11   \n",
       "1       250        11        61        11        11        62        11   \n",
       "2       190        22        62        14        11       135        11   \n",
       "3       133        81        11        11        82        17         8   \n",
       "4       742        18       242        18        18       166        74   \n",
       "\n",
       "   ADK5M011  ADK5M012  ADK5M013  ADK5M014  ADK5M015  ADK5M016  ADK5M017  \\\n",
       "0        44        21        21        11        11        11        11   \n",
       "1        62        25        23        11        11        11        11   \n",
       "2       135        11        11        11        11        11        11   \n",
       "3        16       437        33        50        11        11        11   \n",
       "4       149        71        71        18        18        18        18   \n",
       "\n",
       "   ADK5M018  ADK5M019  ADK5M020  ADK5M021  \n",
       "0        11        11        11        11  \n",
       "1         7        11        11        11  \n",
       "2        11        11        11        11  \n",
       "3       456        29        29        11  \n",
       "4        18        18        18        18  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race = pd.read_csv('data/nhgis0007_ds215_20155_2015_tract.csv', encoding='latin-1')\n",
    "race.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your column types\n",
    "\n",
    "I mentioned it above, but make sure `census_tract` is an object (a string) or merging isn't going to be any fun later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "census_tract                   object\n",
       "county_code                    object\n",
       "state_code                     object\n",
       "applicant_sex                  object\n",
       "income                         object\n",
       "loan_amount                    object\n",
       "loan_type                      object\n",
       "property_type                  object\n",
       "occupancy                      object\n",
       "action_type                    object\n",
       "loan_purpose                   object\n",
       "agency_code                    object\n",
       "tract_to_msa_income_percent    object\n",
       "applicant_race                 object\n",
       "co_applicant_sex               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engineering and cleaning up features\n",
    "\n",
    "## Income-related columns\n",
    "\n",
    "> When we plotted the number of applicants, how much money they made and the size of the loan, we found that it skewed to the left, meaning the majority of applicants were clustered on the lower end of the income and loan amount scales. This was especially true for applicants of color. **We took the logarithm transformation of income and loan amount to normalize the distribution of those variables and limit the effect of extreme outliers.**\n",
    "\n",
    "A few of the columns you'll need to calculate yourselves. **Calculate these values and assign them to three new columns.**\n",
    "\n",
    "* Applicant’s adjusted annual income (includes co-applicant income)\n",
    "* Adjusted loan amount\n",
    "* Ratio between the loan amount and the applicant’s income\n",
    "\n",
    "Instead of using the raw income and loan amount, you'll want the log of both income and loan amount. Call these new columns `log_income` and `log_loan_amount`. The third column will be `loan_income_ratio`.\n",
    "\n",
    "* **Tip:** `np.log` gives you the logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>census_tract</th>\n",
       "      <th>county_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>applicant_sex</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>property_type</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>action_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>agency_code</th>\n",
       "      <th>tract_to_msa_income_percent</th>\n",
       "      <th>applicant_race</th>\n",
       "      <th>co_applicant_sex</th>\n",
       "      <th>log_income</th>\n",
       "      <th>log_loan_amount</th>\n",
       "      <th>loan_income_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010100</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>97.09</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3.258097</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.493981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>026400</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>98.27</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.258097</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>1.132219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>028100</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>72.28</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>0.969166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>015800</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>105.87</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4.043051</td>\n",
       "      <td>3.583519</td>\n",
       "      <td>0.886340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>035800</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>139.62</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4.382027</td>\n",
       "      <td>3.526361</td>\n",
       "      <td>0.804733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  census_tract county_code state_code applicant_sex income loan_amount  \\\n",
       "0       010100         101         42             3     26           5   \n",
       "1       026400         101         42             2     26          40   \n",
       "2       028100         101         42             2     22          20   \n",
       "3       015800         101         42             2     57          36   \n",
       "4       035800         101         42             1     80          34   \n",
       "\n",
       "  loan_type property_type occupancy action_type loan_purpose agency_code  \\\n",
       "0         1             1         1           4            2           5   \n",
       "1         1             1         1           4            2           5   \n",
       "2         1             1         1           5            2           5   \n",
       "3         1             1         1           5            3           5   \n",
       "4         1             1         1           1            3           5   \n",
       "\n",
       "  tract_to_msa_income_percent applicant_race co_applicant_sex  log_income  \\\n",
       "0                       97.09              6                5    3.258097   \n",
       "1                       98.27              3                5    3.258097   \n",
       "2                       72.28              6                5    3.091042   \n",
       "3                      105.87              6                5    4.043051   \n",
       "4                      139.62              5                2    4.382027   \n",
       "\n",
       "   log_loan_amount  loan_income_ratio  \n",
       "0         1.609438           0.493981  \n",
       "1         3.688879           1.132219  \n",
       "2         2.995732           0.969166  \n",
       "3         3.583519           0.886340  \n",
       "4         3.526361           0.804733  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['log_income'] = np.log(df.income.astype(int))\n",
    "df['log_loan_amount'] = np.log(df.loan_amount.astype(int))\n",
    "df['loan_income_ratio'] = df.log_loan_amount / df.log_income\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-applicants\n",
    "\n",
    "Right now we have a column about the co-applicant's sex (see the codebook for column details). We don't want the sex, though, we're interested in whether there is a co applicant or not. Use the co-applicant's sex to **create a new column called `co_applicant` that is either 'yes', 'no', or 'unknown'.**\n",
    "\n",
    "* **Hint:** If the co-applicant's sex was not provided or is not applicable, count it as unknown.\n",
    "* **Hint:** The easiest way is to use `.replace` on the co-applicant sex column, but store the result in your new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    40154\n",
       "2    13014\n",
       "1     4968\n",
       "3     1529\n",
       "4     1040\n",
       "Name: co_applicant_sex, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.co_applicant_sex.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['co_applicant'] = df.co_applicant_sex.replace({'1': 'Yes', '2': 'Yes', '3': 'Unknown', '4': 'Unknown', '5': 'No'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>census_tract</th>\n",
       "      <th>county_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>applicant_sex</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>property_type</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>action_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>agency_code</th>\n",
       "      <th>tract_to_msa_income_percent</th>\n",
       "      <th>applicant_race</th>\n",
       "      <th>co_applicant_sex</th>\n",
       "      <th>log_income</th>\n",
       "      <th>log_loan_amount</th>\n",
       "      <th>loan_income_ratio</th>\n",
       "      <th>co_applicant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010100</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>97.09</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3.258097</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.493981</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>026400</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>98.27</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.258097</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>1.132219</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>028100</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>72.28</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>0.969166</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>015800</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>105.87</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4.043051</td>\n",
       "      <td>3.583519</td>\n",
       "      <td>0.886340</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>035800</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>139.62</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4.382027</td>\n",
       "      <td>3.526361</td>\n",
       "      <td>0.804733</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  census_tract county_code state_code applicant_sex income loan_amount  \\\n",
       "0       010100         101         42             3     26           5   \n",
       "1       026400         101         42             2     26          40   \n",
       "2       028100         101         42             2     22          20   \n",
       "3       015800         101         42             2     57          36   \n",
       "4       035800         101         42             1     80          34   \n",
       "\n",
       "  loan_type property_type occupancy action_type loan_purpose agency_code  \\\n",
       "0         1             1         1           4            2           5   \n",
       "1         1             1         1           4            2           5   \n",
       "2         1             1         1           5            2           5   \n",
       "3         1             1         1           5            3           5   \n",
       "4         1             1         1           1            3           5   \n",
       "\n",
       "  tract_to_msa_income_percent applicant_race co_applicant_sex  log_income  \\\n",
       "0                       97.09              6                5    3.258097   \n",
       "1                       98.27              3                5    3.258097   \n",
       "2                       72.28              6                5    3.091042   \n",
       "3                      105.87              6                5    4.043051   \n",
       "4                      139.62              5                2    4.382027   \n",
       "\n",
       "   log_loan_amount  loan_income_ratio co_applicant  \n",
       "0         1.609438           0.493981           No  \n",
       "1         3.688879           1.132219           No  \n",
       "2         2.995732           0.969166           No  \n",
       "3         3.583519           0.886340           No  \n",
       "4         3.526361           0.804733          Yes  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter loan applicants\n",
    "\n",
    "If you read the whitepaper - `lending_disparities_whitepaper_180214.pdf` - many filters are used to get to the target dataset for analysis.\n",
    "\n",
    "> **Loan type**\n",
    ">\n",
    "> While we recognize the substantial presence of applicants of color in the FHA market, we focused on conventional home loans for several reasons.\n",
    "\n",
    "> **Property type**\n",
    ">\n",
    "> Prospective borrowers submit loan applications for various types of structures: one- to four-unit properties, multifamily properties and manufactured homes. For this analysis, we focused on one- to four-unit properties.\n",
    "\n",
    "> **Occupancy**\n",
    ">\n",
    "> We included only borrowers who said they planned to live in the house they were looking to buy. We did this to exclude developers or individuals who were buying property as an investment or to subsequently flip it.\n",
    "\n",
    "> **Action Type**\n",
    ">\n",
    "> We wanted to look at the reasons lending institutions deny people a mortgage. After conversations with former officials at HUD, we decided to include only those applications that resulted in originations (action type 1) or denials (action type 3)\n",
    "\n",
    "> **Income**\n",
    ">\n",
    "> An applicant’s income isn’t always reported in the data. In other cases, the data cuts off any incomes over \\\\$9.9 million and any loan amounts over \\\\$99.9 million, meaning there’s a value in the database, but it’s not precise. We focused only on those records where income and loan amount have an accurate estimation. This meant discarding about 1 percent of all conventional home loans in the country for 2016. [Note: I already edited this]\n",
    ">\n",
    "> When we plotted the number of applicants, how much money they made and the size of the loan, we found that it skewed to the left, meaning the majority of applicants were clustered on the lower end of the income and loan amount scales. This was especially true for applicants of color. We took the logarithm transformation of income and loan amount to normalize the distribution of those variables and limit the effect of extreme outliers.\n",
    "\n",
    "> **Lien status**\n",
    ">\n",
    "> We included all cases in our analysis regardless of lien status.\n",
    "\n",
    "> **Race and ethnicity**\n",
    ">\n",
    "> At first, we looked at race separate from ethnicity, but that approach introduced too many instances in which​ ​either the ethnicity or race was unknown. So we decided to combine race and ethnicity. Applicants who marked their ethnicity as Hispanic were grouped together as Hispanic/Latino regardless of race. Non-Hispanic applicants, as well as those who didn’t provide an ethnicity, were grouped together by race: non-Hispanic white, non-Hispanic black, etc. **[Note: This has already been taken care of]**\n",
    "\n",
    "> **Loan purpose**\n",
    ">\n",
    "> We decided to look at home purchase, home improvement and refinance loans separately from each other. [Note: please look at **home purchase** loans.]\n",
    "\n",
    "Use the text above (it's from the whitepaper) and the **2015HMDACodeSheet.pdf** code book to filter the dataset.\n",
    "\n",
    "* **Tip:** there should be between 5-8 filters, depending on how you write them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loan \n",
    "df = df[df.loan_type.str.contains('1')]\n",
    "\n",
    "# property \n",
    "df = df[df.property_type.str.contains('1')]\n",
    "\n",
    "# occupancy\n",
    "df = df[df.occupancy.str.contains('1')]\n",
    "\n",
    "# action \n",
    "df = df[df.action_type.str.contains('1|3')]\n",
    "\n",
    "# purpose\n",
    "purchase_df = df[df.loan_purpose.str.contains('1')]\n",
    "improve_df = df[df.loan_purpose.str.contains('2')]\n",
    "refinance_df = df[df.loan_purpose.str.contains('3')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_df = purchase_df.copy()\n",
    "purchase_df = purchase_df.drop(['loan_type','property_type','occupancy','loan_purpose'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're done filtering, save your dataframe as a \"copy\" with `df = df.copy()` (if it's called `df`, of course). This will prevent irritating warnings when you're trying to create new columns.\n",
    "\n",
    "### Confirm that you have 10,107 loans with 19 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10107, 15)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchase_df.shape\n",
    "#Dropped 4 columns above to avoid repetition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a \"loan denied\" column\n",
    "\n",
    "Right now the `action_type` category reflects whether the loan was granted or not, and either has a value of `1` or `3`.\n",
    "\n",
    "Create a new column called `loan_denied`, where the value is `0` if the loan was accepted and `1` if the loan was denied. **This will be our target for the machine learning algorithm.**\n",
    "\n",
    "* **Tip:** You should have 8,878 successful loans and 1,229 denied loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8878\n",
       "1    1229\n",
       "Name: loan_denied, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchase_df['loan_denied'] = purchase_df.action_type.replace({'1':'0', '3':'1'}).astype(int)\n",
    "purchase_df.loan_denied.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with categorical variables\n",
    "\n",
    "Let's go ahead and take a look at our categorical variables:\n",
    "\n",
    "* Applicant sex (male, female, na)\n",
    "* Applicant race\n",
    "* Mortgage agency\n",
    "* Co-applicant (yes, no, unknown)\n",
    "\n",
    "Before we do anything crazy, let's use the codebook to turn them into strings.\n",
    "\n",
    "* **Tip:** We already did this with the `co_applicant` column, you only need to do the rest\n",
    "* **Tip:** Just use `.replace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_df.applicant_sex.replace({'1':'Male', '2':'Female', '3':'unknown'}, inplace=True)\n",
    "purchase_df.applicant_race.replace({'1':'NA', '2':'Asian', '3':'Black', '4':'PI', '5':'White', '6':'Unknown', '99':'Hisp'}, inplace=True)\n",
    "purchase_df.co_applicant_sex.replace({'1':'Male', '2':'Female', '3':'Unknown', '4':'Unknown', '5':'None'}, inplace=True)\n",
    "purchase_df.agency_code.replace({'1':'OCC', '2':'FRS', '3':'FDIC', '5':'NCUA', '7':'HUD', '9':'CFPB'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>census_tract</th>\n",
       "      <th>county_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>applicant_sex</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>action_type</th>\n",
       "      <th>agency_code</th>\n",
       "      <th>tract_to_msa_income_percent</th>\n",
       "      <th>applicant_race</th>\n",
       "      <th>co_applicant_sex</th>\n",
       "      <th>log_income</th>\n",
       "      <th>log_loan_amount</th>\n",
       "      <th>loan_income_ratio</th>\n",
       "      <th>co_applicant</th>\n",
       "      <th>loan_denied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>401900</td>\n",
       "      <td>045</td>\n",
       "      <td>42</td>\n",
       "      <td>Female</td>\n",
       "      <td>59</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>OCC</td>\n",
       "      <td>133.09</td>\n",
       "      <td>White</td>\n",
       "      <td>None</td>\n",
       "      <td>4.077537</td>\n",
       "      <td>4.718499</td>\n",
       "      <td>1.157193</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>409902</td>\n",
       "      <td>045</td>\n",
       "      <td>42</td>\n",
       "      <td>unknown</td>\n",
       "      <td>177</td>\n",
       "      <td>375</td>\n",
       "      <td>1</td>\n",
       "      <td>OCC</td>\n",
       "      <td>208.56</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>5.176150</td>\n",
       "      <td>5.926926</td>\n",
       "      <td>1.145045</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>410200</td>\n",
       "      <td>045</td>\n",
       "      <td>42</td>\n",
       "      <td>Male</td>\n",
       "      <td>150</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "      <td>OCC</td>\n",
       "      <td>215.35</td>\n",
       "      <td>White</td>\n",
       "      <td>None</td>\n",
       "      <td>5.010635</td>\n",
       "      <td>5.942799</td>\n",
       "      <td>1.186037</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>031200</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>Female</td>\n",
       "      <td>65</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>OCC</td>\n",
       "      <td>93.11</td>\n",
       "      <td>Asian</td>\n",
       "      <td>None</td>\n",
       "      <td>4.174387</td>\n",
       "      <td>4.912655</td>\n",
       "      <td>1.176857</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>403601</td>\n",
       "      <td>045</td>\n",
       "      <td>42</td>\n",
       "      <td>Female</td>\n",
       "      <td>55</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "      <td>OCC</td>\n",
       "      <td>141.83</td>\n",
       "      <td>Asian</td>\n",
       "      <td>None</td>\n",
       "      <td>4.007333</td>\n",
       "      <td>5.278115</td>\n",
       "      <td>1.317114</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   census_tract county_code state_code applicant_sex income loan_amount  \\\n",
       "42       401900         045         42        Female     59         112   \n",
       "43       409902         045         42       unknown    177         375   \n",
       "46       410200         045         42          Male    150         381   \n",
       "48       031200         101         42        Female     65         136   \n",
       "51       403601         045         42        Female     55         196   \n",
       "\n",
       "   action_type agency_code tract_to_msa_income_percent applicant_race  \\\n",
       "42           1         OCC                      133.09          White   \n",
       "43           1         OCC                      208.56        Unknown   \n",
       "46           1         OCC                      215.35          White   \n",
       "48           1         OCC                       93.11          Asian   \n",
       "51           1         OCC                      141.83          Asian   \n",
       "\n",
       "   co_applicant_sex  log_income  log_loan_amount  loan_income_ratio  \\\n",
       "42             None    4.077537         4.718499           1.157193   \n",
       "43          Unknown    5.176150         5.926926           1.145045   \n",
       "46             None    5.010635         5.942799           1.186037   \n",
       "48             None    4.174387         4.912655           1.176857   \n",
       "51             None    4.007333         5.278115           1.317114   \n",
       "\n",
       "   co_applicant  loan_denied  \n",
       "42           No            0  \n",
       "43      Unknown            0  \n",
       "46           No            0  \n",
       "48           No            0  \n",
       "51           No            0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchase_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-check these columns match these values in the first three rows (and yes, you should have a lot of other columns, too).\n",
    "\n",
    "|applicant_sex|agency_code|applicant_race|co_applicant|\n",
    "|---|---|---|---|\n",
    "|female|OCC|white|no|\n",
    "|na|OCC|na|unknown|\n",
    "|male|OCC|white|no|\n",
    "\n",
    "## Dummy variables\n",
    "\n",
    "Let's say we're at the end of the homework, and we have a column called `sex`, where `0` is female and `1` is male. After we've done our regression, we can look at the coefficient/odds ratio for `sex` and say something like **\"being male gives you a 1.5x odds of being denied a loan.\"**\n",
    "\n",
    "We can say this because we're looking at one column, and changing `sex` from `0` to `1` would turn the applicant male and give them a 1.5x chance of being denied (the odds ratio).\n",
    "\n",
    "**But let's say we're looking at a column called `race` instead.** We could do the same `0`/`1` thing with white/minority, but what about white/black/asian? If we try to give them `0`/`1`/`2` our coefficient/odds ratio interpreation stops working, because we don't have a nice True/False dichotomy any more, it's now a *real number*.\n",
    "\n",
    "* `0`: White\n",
    "* `1`: Black\n",
    "* `2`: Asian\n",
    "\n",
    "Usually with numbers you can say \"...for every increase of 1...\", but we can't anymore - changing from White to Black (+1) isn't the same as changing from Black to Asian (+1). And you can't subtract Black from Asian to get White. And no, you also can't average together White and Asian to get Black. Just recognize that these aren't numbers, they're categories!\n",
    "\n",
    "**How can we turn races off and on like we can turn the `sex` variable off and on?** A good option is to make *a `0`/`1` column for each race*. We can then flip each race off and on. These are called **dummy variables**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_1</th>\n",
       "      <th>race_2</th>\n",
       "      <th>race_3</th>\n",
       "      <th>race_4</th>\n",
       "      <th>race_5</th>\n",
       "      <th>race_6</th>\n",
       "      <th>race_7</th>\n",
       "      <th>race_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    race_1  race_2  race_3  race_4  race_5  race_6  race_7  race_99\n",
       "4        0       0       0       0       1       0       0        0\n",
       "6        0       0       0       0       0       1       0        0\n",
       "10       0       0       1       0       0       0       0        0\n",
       "11       0       0       1       0       0       0       0        0\n",
       "12       0       0       0       0       0       1       0        0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df.applicant_race, prefix='race').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to take up a lot of space, but it works a lot better.\n",
    "\n",
    "* The first person is white, so they have a `1` for white and a `0` for every other race\n",
    "* The second person is N/A, so they have a `1` for N/A and a `0` for every other race\n",
    "* The next three are white, asian, and asian, so they have a `1` under the appropriate column.\n",
    "\n",
    "When you're looking at the regression output, each column has its own coefficient (and odds ratio). Since each race now has a column, **each race will also have its own odds ratio.** Asian would have one, Black would have one, Latino would have one - now we can look at the effect of each race separately. For example, you could then say something like \"being Asian (e.g., `race_asian` going from `0` to `1`) gives you a 1.2x greater chance of being denied, and being Black gets you a 2.1x chance of being denied.\"\n",
    "\n",
    "And no, you're never going to have more than one `1` in a row at the same time.\n",
    "\n",
    "After you've created your dummy variables, there's one more step which has a real fun name: **one-hot encoding.**\n",
    "\n",
    "### One-hot encoding\n",
    "\n",
    "When we have two sexes - male and female - we can flip between them with one binary digit, `0` and `1`.\n",
    "\n",
    "If we had three races - White, Asian, Black - using `pd.get_dummies` would make three columns, which makes sense on the surface. But why can we put TWO values in ONE column for sex, and it takes THREE columns for the THREE race values?\n",
    "\n",
    "The truth is, it doesn't have to!\n",
    "\n",
    "Instead of having three columns, we're only going to have two: **asian and black**. And if both of them are `0`? The applicant is white! This is called a **reference category**, and it means **the coefficients/odds ratios for asian and black are in reference to a white applicant.** So it isn't \"being black gets you a 2.1x chance of being denied,\" it's *being black gets you a 2.1x chance of being denied compared to a white person*. For example:\n",
    "\n",
    "|race_asian|race_black|person's race|\n",
    "|---|---|---|\n",
    "|1|0|Asian|\n",
    "|0|1|Black|\n",
    "|0|0|White|\n",
    "|1|1|Not possible if your source is a single race column|\n",
    "\n",
    "To create a one-hot encoded variable with a reference category, you write code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_Asian</th>\n",
       "      <th>race_Black</th>\n",
       "      <th>race_Hisp</th>\n",
       "      <th>race_NA</th>\n",
       "      <th>race_PI</th>\n",
       "      <th>race_Unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    race_Asian  race_Black  race_Hisp  race_NA  race_PI  race_Unknown\n",
       "42           0           0          0        0        0             0\n",
       "43           0           0          0        0        0             1\n",
       "46           0           0          0        0        0             0\n",
       "48           1           0          0        0        0             0\n",
       "51           1           0          0        0        0             0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(purchase_df.applicant_race, prefix='race').drop('race_White', axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We usually use `.drop(columns=...)` to drop columns, but I'm using `axis=1` here because you should be familiar with it\n",
    "\n",
    "### Make a one-hot encoded `sex` category with `female` as the reference category\n",
    "\n",
    "You should end up with two columns: `sex_male` and `sex_na`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex_Male</th>\n",
       "      <th>sex_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sex_Male  sex_unknown\n",
       "42         0            0\n",
       "43         0            1\n",
       "46         1            0\n",
       "48         0            0\n",
       "51         0            0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.get_dummies(df.applicant_sex, prefix='sex').drop('sex_female', axis=1).head()\n",
    "pd.get_dummies(purchase_df.applicant_sex, prefix='sex').drop('sex_Female', axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using one-hot encoded columns\n",
    "\n",
    "Since these one-hot encoded variables are standalone dataframes, we eventually need to combine them into our original dataframe.\n",
    "\n",
    "We have four categorical variables - sex, race, co-applicant, and the loan agency - so we need you to **make four one-hot encoded variables**. Name them like this:\n",
    "\n",
    "* `dummies_sex` - reference category of white\n",
    "* `dummies_race` - reference category of female\n",
    "* `dummies_co_applicant` - reference category of no\n",
    "* `dummies_agency` - reference category of FDIC\n",
    "\n",
    "Typically your reference category is the most common category, because it makes for the most interesting comparisons.\n",
    "\n",
    "> **Tip:** if you're cutting and pasting from above, watch out for `.head()`\n",
    ">\n",
    "> **Tip:** After you've made them, use `.head(2)` to check the first couple rows of each to make sure they look okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_race = pd.get_dummies(purchase_df.applicant_race, prefix='race').drop('race_White', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_sex = pd.get_dummies(purchase_df.applicant_sex, prefix='sex').drop('sex_Female', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_co_applicant = pd.get_dummies(purchase_df.co_applicant, prefix='co_app').drop('co_app_No', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_agency = pd.get_dummies(purchase_df.agency_code, prefix='agency').drop('agency_FDIC', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up our old dataframe\n",
    "\n",
    "Take a look at your original dataframe real quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>census_tract</th>\n",
       "      <th>county_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>applicant_sex</th>\n",
       "      <th>income</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>action_type</th>\n",
       "      <th>agency_code</th>\n",
       "      <th>tract_to_msa_income_percent</th>\n",
       "      <th>applicant_race</th>\n",
       "      <th>co_applicant_sex</th>\n",
       "      <th>log_income</th>\n",
       "      <th>log_loan_amount</th>\n",
       "      <th>loan_income_ratio</th>\n",
       "      <th>co_applicant</th>\n",
       "      <th>loan_denied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>401900</td>\n",
       "      <td>045</td>\n",
       "      <td>42</td>\n",
       "      <td>Female</td>\n",
       "      <td>59</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>OCC</td>\n",
       "      <td>133.09</td>\n",
       "      <td>White</td>\n",
       "      <td>None</td>\n",
       "      <td>4.077537</td>\n",
       "      <td>4.718499</td>\n",
       "      <td>1.157193</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>409902</td>\n",
       "      <td>045</td>\n",
       "      <td>42</td>\n",
       "      <td>unknown</td>\n",
       "      <td>177</td>\n",
       "      <td>375</td>\n",
       "      <td>1</td>\n",
       "      <td>OCC</td>\n",
       "      <td>208.56</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>5.176150</td>\n",
       "      <td>5.926926</td>\n",
       "      <td>1.145045</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>410200</td>\n",
       "      <td>045</td>\n",
       "      <td>42</td>\n",
       "      <td>Male</td>\n",
       "      <td>150</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "      <td>OCC</td>\n",
       "      <td>215.35</td>\n",
       "      <td>White</td>\n",
       "      <td>None</td>\n",
       "      <td>5.010635</td>\n",
       "      <td>5.942799</td>\n",
       "      <td>1.186037</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>031200</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>Female</td>\n",
       "      <td>65</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>OCC</td>\n",
       "      <td>93.11</td>\n",
       "      <td>Asian</td>\n",
       "      <td>None</td>\n",
       "      <td>4.174387</td>\n",
       "      <td>4.912655</td>\n",
       "      <td>1.176857</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>403601</td>\n",
       "      <td>045</td>\n",
       "      <td>42</td>\n",
       "      <td>Female</td>\n",
       "      <td>55</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "      <td>OCC</td>\n",
       "      <td>141.83</td>\n",
       "      <td>Asian</td>\n",
       "      <td>None</td>\n",
       "      <td>4.007333</td>\n",
       "      <td>5.278115</td>\n",
       "      <td>1.317114</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   census_tract county_code state_code applicant_sex income loan_amount  \\\n",
       "42       401900         045         42        Female     59         112   \n",
       "43       409902         045         42       unknown    177         375   \n",
       "46       410200         045         42          Male    150         381   \n",
       "48       031200         101         42        Female     65         136   \n",
       "51       403601         045         42        Female     55         196   \n",
       "\n",
       "   action_type agency_code tract_to_msa_income_percent applicant_race  \\\n",
       "42           1         OCC                      133.09          White   \n",
       "43           1         OCC                      208.56        Unknown   \n",
       "46           1         OCC                      215.35          White   \n",
       "48           1         OCC                       93.11          Asian   \n",
       "51           1         OCC                      141.83          Asian   \n",
       "\n",
       "   co_applicant_sex  log_income  log_loan_amount  loan_income_ratio  \\\n",
       "42             None    4.077537         4.718499           1.157193   \n",
       "43          Unknown    5.176150         5.926926           1.145045   \n",
       "46             None    5.010635         5.942799           1.186037   \n",
       "48             None    4.174387         4.912655           1.176857   \n",
       "51             None    4.007333         5.278115           1.317114   \n",
       "\n",
       "   co_applicant  loan_denied  \n",
       "42           No            0  \n",
       "43      Unknown            0  \n",
       "46           No            0  \n",
       "48           No            0  \n",
       "51           No            0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchase_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need all of those columns! If we look at the list of columns we'll be using for the regression:\n",
    "\n",
    "* Race/Ethnicity\n",
    "* Sex\n",
    "* Whether there was a co-applicant\n",
    "* Applicant’s annual income (includes co-applicant income)\n",
    "* Loan amount\n",
    "* Ratio between the loan amount and the applicant’s income\n",
    "* Ratio between the median income of the census tract and the median income of the metro area\n",
    "* Racial and ethnic breakdown by percentage for each census tract\n",
    "* Regulating agency of the lending institution\n",
    "\n",
    "We can keep anything in that list, and remove everything else. For example, we can drop the variables we used to create the dummy variables, as we'll be adding the one-hot encoded versions in for the next step.\n",
    "\n",
    "For \"Racial and ethnic breakdown by percentage for each census tract\" we'll need to join with some census data later, so we need to also keep census tract, county code and state code.\n",
    "\n",
    "**Build a new dataframe with only the columns we're interested in, call it `numeric`.** We're calling it `numeric` because it's mostly numeric columns after the categorical ones have been removed.\n",
    "\n",
    "> **Tip:** You can either use `.drop(columns=` to remove unwanted columns or `df = df[['col1', 'col2', ... 'col12']]` to only select the ones you're interseted in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = purchase_df[['census_tract','county_code','state_code','tract_to_msa_income_percent','log_income','log_loan_amount','loan_income_ratio','loan_denied']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that `numeric` has 8 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10107, 8)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining our features\n",
    "\n",
    "We now have 1 dataframe of numeric features (and some merge columns), and 4 one-hot-encoded variables (each with their own dataframe). Combine all five dataframes into one large dataframe called `loan_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>census_tract</th>\n",
       "      <th>county_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>tract_to_msa_income_percent</th>\n",
       "      <th>log_income</th>\n",
       "      <th>log_loan_amount</th>\n",
       "      <th>loan_income_ratio</th>\n",
       "      <th>loan_denied</th>\n",
       "      <th>race_Asian</th>\n",
       "      <th>race_Black</th>\n",
       "      <th>race_Hisp</th>\n",
       "      <th>race_NA</th>\n",
       "      <th>race_PI</th>\n",
       "      <th>race_Unknown</th>\n",
       "      <th>sex_Male</th>\n",
       "      <th>sex_unknown</th>\n",
       "      <th>co_app_Unknown</th>\n",
       "      <th>co_app_Yes</th>\n",
       "      <th>agency_CFPB</th>\n",
       "      <th>agency_FRS</th>\n",
       "      <th>agency_HUD</th>\n",
       "      <th>agency_NCUA</th>\n",
       "      <th>agency_OCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>401900</td>\n",
       "      <td>045</td>\n",
       "      <td>42</td>\n",
       "      <td>133.09</td>\n",
       "      <td>4.077537</td>\n",
       "      <td>4.718499</td>\n",
       "      <td>1.157193</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>409902</td>\n",
       "      <td>045</td>\n",
       "      <td>42</td>\n",
       "      <td>208.56</td>\n",
       "      <td>5.176150</td>\n",
       "      <td>5.926926</td>\n",
       "      <td>1.145045</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>410200</td>\n",
       "      <td>045</td>\n",
       "      <td>42</td>\n",
       "      <td>215.35</td>\n",
       "      <td>5.010635</td>\n",
       "      <td>5.942799</td>\n",
       "      <td>1.186037</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>031200</td>\n",
       "      <td>101</td>\n",
       "      <td>42</td>\n",
       "      <td>93.11</td>\n",
       "      <td>4.174387</td>\n",
       "      <td>4.912655</td>\n",
       "      <td>1.176857</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>403601</td>\n",
       "      <td>045</td>\n",
       "      <td>42</td>\n",
       "      <td>141.83</td>\n",
       "      <td>4.007333</td>\n",
       "      <td>5.278115</td>\n",
       "      <td>1.317114</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   census_tract county_code state_code tract_to_msa_income_percent  \\\n",
       "42       401900         045         42                      133.09   \n",
       "43       409902         045         42                      208.56   \n",
       "46       410200         045         42                      215.35   \n",
       "48       031200         101         42                       93.11   \n",
       "51       403601         045         42                      141.83   \n",
       "\n",
       "    log_income  log_loan_amount  loan_income_ratio  loan_denied  race_Asian  \\\n",
       "42    4.077537         4.718499           1.157193            0           0   \n",
       "43    5.176150         5.926926           1.145045            0           0   \n",
       "46    5.010635         5.942799           1.186037            0           0   \n",
       "48    4.174387         4.912655           1.176857            0           1   \n",
       "51    4.007333         5.278115           1.317114            0           1   \n",
       "\n",
       "    race_Black  race_Hisp  race_NA  race_PI  race_Unknown  sex_Male  \\\n",
       "42           0          0        0        0             0         0   \n",
       "43           0          0        0        0             1         0   \n",
       "46           0          0        0        0             0         1   \n",
       "48           0          0        0        0             0         0   \n",
       "51           0          0        0        0             0         0   \n",
       "\n",
       "    sex_unknown  co_app_Unknown  co_app_Yes  agency_CFPB  agency_FRS  \\\n",
       "42            0               0           0            0           0   \n",
       "43            1               1           0            0           0   \n",
       "46            0               0           0            0           0   \n",
       "48            0               0           0            0           0   \n",
       "51            0               0           0            0           0   \n",
       "\n",
       "    agency_HUD  agency_NCUA  agency_OCC  \n",
       "42           0            0           1  \n",
       "43           0            0           1  \n",
       "46           0            0           1  \n",
       "48           0            0           1  \n",
       "51           0            0           1  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_features = pd.concat([numeric, dummies_race, dummies_sex, dummies_co_applicant, dummies_agency], axis=1)\n",
    "loan_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that `loan_features` has 10,107 rows and 23 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10107, 23)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Census data\n",
    "\n",
    "Now we just need the final piece to the puzzle, the census data. Read in the census data file, calling the dataframe `census`.\n",
    "\n",
    "> **Tip:** As always, be sure to read the tract column in as a string. Interestingly, this time we _don't_ need to worry about the state or county codes in the same way.\n",
    ">\n",
    "> **Tip:** You're going to encounter a problem that you find every time you read in a file from the US government!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_code</th>\n",
       "      <th>county_code</th>\n",
       "      <th>census_tract</th>\n",
       "      <th>Total</th>\n",
       "      <th>NoHisp_All</th>\n",
       "      <th>NoHisp_White</th>\n",
       "      <th>NoHisp_Black</th>\n",
       "      <th>NoHisp_NA</th>\n",
       "      <th>NoHisp_Asian</th>\n",
       "      <th>NoHisp_PI</th>\n",
       "      <th>NoHisp_Other</th>\n",
       "      <th>NoHisp_Multi1</th>\n",
       "      <th>NoHisp_Multi2</th>\n",
       "      <th>NoHisp_Multi3</th>\n",
       "      <th>Hisp_All</th>\n",
       "      <th>Hisp_White</th>\n",
       "      <th>Hisp_Black</th>\n",
       "      <th>Hisp_NA</th>\n",
       "      <th>Hisp_Asian</th>\n",
       "      <th>Hisp_PI</th>\n",
       "      <th>Hisp_Other</th>\n",
       "      <th>Hisp_Multi1</th>\n",
       "      <th>Hisp_Multi2</th>\n",
       "      <th>Hisp_Multi3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50802</th>\n",
       "      <td>39</td>\n",
       "      <td>025</td>\n",
       "      <td>040900</td>\n",
       "      <td>5840</td>\n",
       "      <td>5829</td>\n",
       "      <td>5744</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19541</th>\n",
       "      <td>13</td>\n",
       "      <td>135</td>\n",
       "      <td>050315</td>\n",
       "      <td>5724</td>\n",
       "      <td>4212</td>\n",
       "      <td>1898</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>1512</td>\n",
       "      <td>814</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>631</td>\n",
       "      <td>55</td>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36885</th>\n",
       "      <td>28</td>\n",
       "      <td>015</td>\n",
       "      <td>950200</td>\n",
       "      <td>5371</td>\n",
       "      <td>5354</td>\n",
       "      <td>2732</td>\n",
       "      <td>2564</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64546</th>\n",
       "      <td>48</td>\n",
       "      <td>201</td>\n",
       "      <td>533300</td>\n",
       "      <td>6452</td>\n",
       "      <td>5526</td>\n",
       "      <td>18</td>\n",
       "      <td>5508</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>926</td>\n",
       "      <td>778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34891</th>\n",
       "      <td>26</td>\n",
       "      <td>163</td>\n",
       "      <td>500500</td>\n",
       "      <td>1649</td>\n",
       "      <td>1635</td>\n",
       "      <td>10</td>\n",
       "      <td>1603</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      state_code county_code census_tract  Total  NoHisp_All  NoHisp_White  \\\n",
       "50802         39         025       040900   5840        5829          5744   \n",
       "19541         13         135       050315   5724        4212          1898   \n",
       "36885         28         015       950200   5371        5354          2732   \n",
       "64546         48         201       533300   6452        5526            18   \n",
       "34891         26         163       500500   1649        1635            10   \n",
       "\n",
       "       NoHisp_Black  NoHisp_NA  NoHisp_Asian  NoHisp_PI  NoHisp_Other  \\\n",
       "50802            55          0             0          0             0   \n",
       "19541          2005          0           108          0            28   \n",
       "36885          2564          0             0          0             0   \n",
       "64546          5508          0             0          0             0   \n",
       "34891          1603          5             0          0             0   \n",
       "\n",
       "       NoHisp_Multi1  NoHisp_Multi2  NoHisp_Multi3  Hisp_All  Hisp_White  \\\n",
       "50802             30              0             30        11           5   \n",
       "19541            173              0            173      1512         814   \n",
       "36885             58              0             58        17           7   \n",
       "64546              0              0              0       926         778   \n",
       "34891             17              0             17        14           0   \n",
       "\n",
       "       Hisp_Black  Hisp_NA  Hisp_Asian  Hisp_PI  Hisp_Other  Hisp_Multi1  \\\n",
       "50802           0        0           0        0           6            0   \n",
       "19541          12        0           0        0         631           55   \n",
       "36885           0        0           0        0           0           10   \n",
       "64546           0        0           0        0         147            1   \n",
       "34891          14        0           0        0           0            0   \n",
       "\n",
       "       Hisp_Multi2  Hisp_Multi3  \n",
       "50802            0            0  \n",
       "19541           42           13  \n",
       "36885           10            0  \n",
       "64546            0            1  \n",
       "34891            0            0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['STATEA',\n",
    "           'COUNTYA',\n",
    "           'TRACTA',\n",
    "           'ADK5E001',\n",
    "           'ADK5E002','ADK5E003','ADK5E004','ADK5E005','ADK5E006','ADK5E007','ADK5E008','ADK5E009','ADK5E010','ADK5E011',\n",
    "           'ADK5E012','ADK5E013','ADK5E014','ADK5E015','ADK5E016','ADK5E017','ADK5E018','ADK5E019','ADK5E020','ADK5E021']\n",
    "census = pd.read_csv('data/nhgis0007_ds215_20155_2015_tract.csv', encoding='latin-1', usecols=columns, dtype={'STATEA':str,'COUNTYA':str,'TRACTA':str})\n",
    "census.rename(columns={'STATEA':'state_code',\n",
    "                       'COUNTYA':'county_code',\n",
    "                       'TRACTA':'census_tract',\n",
    "                       'ADK5E001':'Total',\n",
    "                       'ADK5E002':'NoHisp_All',\n",
    "                       'ADK5E003':'NoHisp_White',\n",
    "                       'ADK5E004':'NoHisp_Black',\n",
    "                       'ADK5E005':'NoHisp_NA',\n",
    "                       'ADK5E006':'NoHisp_Asian',\n",
    "                       'ADK5E007':'NoHisp_PI',\n",
    "                       'ADK5E008':'NoHisp_Other',\n",
    "                       'ADK5E009':'NoHisp_Multi1',\n",
    "                       'ADK5E010':'NoHisp_Multi2',\n",
    "                       'ADK5E011':'NoHisp_Multi3',\n",
    "                       'ADK5E012':'Hisp_All',\n",
    "                       'ADK5E013':'Hisp_White',\n",
    "                       'ADK5E014':'Hisp_Black',\n",
    "                       'ADK5E015':'Hisp_NA',\n",
    "                       'ADK5E016':'Hisp_Asian',\n",
    "                       'ADK5E017':'Hisp_PI',\n",
    "                       'ADK5E018':'Hisp_Other',\n",
    "                       'ADK5E019':'Hisp_Multi1',\n",
    "                       'ADK5E020':'Hisp_Multi2',\n",
    "                       'ADK5E021':'Hisp_Multi3'\n",
    "                      }, inplace=True)\n",
    "census.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename some columns\n",
    "\n",
    "If you like to keep your data extra clean, feel free to rename the columns you're interested in. If not, feel free to skip it!\n",
    "\n",
    "> **Tip:** Make sure you're using the estimates columns, not the margin of error columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy and pasted from a previous hm :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computed columns\n",
    "\n",
    "According to Reveal's regression output, you'll want to create the following columns:\n",
    "\n",
    "* Percent Black in tract\n",
    "* Percent Hispanic/Latino in tract (I hope you know how Hispanic/Latino + census data works by now)\n",
    "* Percent Asian in tract\n",
    "* Percent Native American in tract\n",
    "* Percent Native Hawaiian in tract\n",
    "\n",
    "Notice that we don't include percent white - **because all of the other columns add up to percent white, we ignore it!** It's similar to a reference category.\n",
    "\n",
    "> If we want to use buzzwords here, the technical reason we're not using percent white is called **collinearity.** We'll talk more about it on Friday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovanafleck/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/giovanafleck/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/giovanafleck/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/giovanafleck/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/giovanafleck/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_code</th>\n",
       "      <th>county_code</th>\n",
       "      <th>census_tract</th>\n",
       "      <th>pct_Hisp</th>\n",
       "      <th>pct_Black</th>\n",
       "      <th>pct_NA</th>\n",
       "      <th>pct_Asian</th>\n",
       "      <th>pct_PI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>04</td>\n",
       "      <td>007</td>\n",
       "      <td>001100</td>\n",
       "      <td>43.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>6.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8514</th>\n",
       "      <td>06</td>\n",
       "      <td>065</td>\n",
       "      <td>941100</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45684</th>\n",
       "      <td>36</td>\n",
       "      <td>061</td>\n",
       "      <td>010200</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>9.7</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43890</th>\n",
       "      <td>36</td>\n",
       "      <td>027</td>\n",
       "      <td>130005</td>\n",
       "      <td>10.1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37476</th>\n",
       "      <td>28</td>\n",
       "      <td>151</td>\n",
       "      <td>001100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      state_code county_code census_tract  pct_Hisp  pct_Black  pct_NA  \\\n",
       "1435          04         007       001100      43.8        0.4     6.4   \n",
       "8514          06         065       941100       5.0        0.2     0.1   \n",
       "45684         36         061       010200       3.1        0.4     9.7   \n",
       "43890         36         027       130005      10.1        4.5     0.0   \n",
       "37476         28         151       001100       0.0       99.4     0.0   \n",
       "\n",
       "       pct_Asian  pct_PI  \n",
       "1435         1.5     0.0  \n",
       "8514         1.2     0.0  \n",
       "45684        4.3     0.0  \n",
       "43890        0.0     0.0  \n",
       "37476        0.0     0.0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_features = census[['state_code','county_code','census_tract']]\n",
    "census_features['pct_Hisp'] = ((census.Hisp_All / census.Total) * 100).round(1)\n",
    "census_features['pct_Black'] = ((census.NoHisp_Black / census.Total) * 100).round(1)\n",
    "census_features['pct_NA'] = ((census.NoHisp_NA / census.Total) * 100).round(1)\n",
    "census_features['pct_Asian'] = ((census.NoHisp_Asian / census.Total) * 100).round(1)\n",
    "census_features['pct_PI'] = ((census.NoHisp_PI / census.Total) * 100).round(1)\n",
    "census_features.sample(5)\n",
    "\n",
    "#Lots of red areas - should I be doing something different??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only keep what we need to join and process\n",
    "\n",
    "We're only interested in the percentage columns that we computed. Create a new dataframe called `census_features` that is only those columns along with the one we'll need for joining with the mortgage data.\n",
    "\n",
    "> * **Tip:** Remember we saved state, county and tract codes when working on the loan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that your first few rows look something like this:\n",
    "    \n",
    "|STATEA|COUNTYA|TRACTA|pct_hispanic|pct_black|pct_amer_indian|pct_asian|pct_pac_islander|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|1|1|020100|0.872690|7.700205|0.308008|0.616016|0.000000|\n",
    "|1|1|020200|0.788497|53.293135|0.000000|2.319109|0.000000|\n",
    "|1|1|020300|0.000000|18.564690|0.505391|1.381402|0.269542|\n",
    "|1|1|020400|10.490617|3.662672|1.560027|0.000000|0.000000|\n",
    "|1|1|020500|0.743287|24.844374|0.000000|3.827929|0.000000|\n",
    "\n",
    "Your column headers might be different but your numbers should match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge datasets\n",
    "\n",
    "Merge `loan_features` and `census_features` into a new dataframe called `merged`.\n",
    "\n",
    "Unfortunately something is a little different between our `loan_features` and `census_features` census tract columns. You'll need to fix it before you can merge.\n",
    "\n",
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think it's ok with the cleaning done before (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged = pd.merge(loan_features, census_features, on=['state_code','county_code','census_tract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm you have 10107 rows and 31 columns in the merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10107, 28)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.shape #droped some while cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our final dataframe\n",
    "\n",
    "Drop all of the columns we merged on and save it as `train_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = merged.drop(['census_tract','county_code','state_code'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that `train_df` has 10107 rows and 25 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10107, 25)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final cleanup\n",
    "\n",
    "Because we can't have missing data before we run a regression, check the size of `train_df`, then drop any missing data and check the size again. **Confirm you don't lose any rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10107, 25)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing our regression\n",
    "\n",
    "## Try with statsmodels\n",
    "\n",
    "First try to run a linear regression with statsmodels, because even though sometimes it complains and breaks, the output just looks *so nice*. Instead of `sm.OLS` we'll use `sm.Logit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.352023\n",
      "         Iterations 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovanafleck/.pyenv/versions/3.6.5/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>loan_denied</td>   <th>  No. Observations:  </th>  <td> 10107</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td> 10098</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     8</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 24 Jul 2019</td> <th>  Pseudo R-squ.:     </th>  <td>0.04884</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>12:31:55</td>     <th>  Log-Likelihood:    </th> <td> -3557.9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -3740.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>4.726e-74</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td>   -0.3858</td> <td>    0.231</td> <td>   -1.671</td> <td> 0.095</td> <td>   -0.838</td> <td>    0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>race_Black</th> <td>    0.7801</td> <td>    0.104</td> <td>    7.468</td> <td> 0.000</td> <td>    0.575</td> <td>    0.985</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>race_Hisp</th>  <td>    0.2033</td> <td>    0.157</td> <td>    1.296</td> <td> 0.195</td> <td>   -0.104</td> <td>    0.511</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>race_Asian</th> <td>    0.2429</td> <td>    0.099</td> <td>    2.447</td> <td> 0.014</td> <td>    0.048</td> <td>    0.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pct_Black</th>  <td>    0.0069</td> <td>    0.001</td> <td>    5.199</td> <td> 0.000</td> <td>    0.004</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pct_Hisp</th>   <td>    0.0033</td> <td>    0.003</td> <td>    1.059</td> <td> 0.290</td> <td>   -0.003</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pct_Asian</th>  <td>    0.0087</td> <td>    0.004</td> <td>    2.097</td> <td> 0.036</td> <td>    0.001</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex_Male</th>   <td>    0.0030</td> <td>    0.064</td> <td>    0.047</td> <td> 0.962</td> <td>   -0.122</td> <td>    0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_income</th> <td>   -0.4472</td> <td>    0.048</td> <td>   -9.246</td> <td> 0.000</td> <td>   -0.542</td> <td>   -0.352</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:            loan_denied   No. Observations:                10107\n",
       "Model:                          Logit   Df Residuals:                    10098\n",
       "Method:                           MLE   Df Model:                            8\n",
       "Date:                Wed, 24 Jul 2019   Pseudo R-squ.:                 0.04884\n",
       "Time:                        12:31:55   Log-Likelihood:                -3557.9\n",
       "converged:                       True   LL-Null:                       -3740.6\n",
       "Covariance Type:            nonrobust   LLR p-value:                 4.726e-74\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -0.3858      0.231     -1.671      0.095      -0.838       0.067\n",
       "race_Black     0.7801      0.104      7.468      0.000       0.575       0.985\n",
       "race_Hisp      0.2033      0.157      1.296      0.195      -0.104       0.511\n",
       "race_Asian     0.2429      0.099      2.447      0.014       0.048       0.437\n",
       "pct_Black      0.0069      0.001      5.199      0.000       0.004       0.009\n",
       "pct_Hisp       0.0033      0.003      1.059      0.290      -0.003       0.009\n",
       "pct_Asian      0.0087      0.004      2.097      0.036       0.001       0.017\n",
       "sex_Male       0.0030      0.064      0.047      0.962      -0.122       0.128\n",
       "log_income    -0.4472      0.048     -9.246      0.000      -0.542      -0.352\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_df[['race_Black','race_Hisp','race_Asian','pct_Black','pct_Hisp','pct_Asian','sex_Male','log_income']]\n",
    "X = sm.add_constant(X)\n",
    "y = train_df['loan_denied']               \n",
    "\n",
    "mod = sm.Logit(y,X)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try again with sci-kit learn\n",
    "\n",
    "But I'll be honest, I like sklearn a *lot lot lot* better. Using the coefficient to build a dataframe just seems so *nice*.\n",
    "\n",
    "> **Tip:** When you build your model, use `LogisticRegression(C=1e9, solver='lbfgs', max_iter=4000)` - for if you don't increase `max_iter` (how long/hard it works) it'll complain it can't find an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovanafleck/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_Black</th>\n",
       "      <th>race_Hisp</th>\n",
       "      <th>race_Asian</th>\n",
       "      <th>pct_Black</th>\n",
       "      <th>pct_Hisp</th>\n",
       "      <th>pct_Asian</th>\n",
       "      <th>sex_Male</th>\n",
       "      <th>log_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.181656</td>\n",
       "      <td>1.225276</td>\n",
       "      <td>1.274949</td>\n",
       "      <td>1.006887</td>\n",
       "      <td>1.003322</td>\n",
       "      <td>1.008754</td>\n",
       "      <td>1.00301</td>\n",
       "      <td>0.639398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   race_Black  race_Hisp  race_Asian  pct_Black  pct_Hisp  pct_Asian  \\\n",
       "0    2.181656   1.225276    1.274949   1.006887  1.003322   1.008754   \n",
       "\n",
       "   sex_Male  log_income  \n",
       "0   1.00301    0.639398  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = ['race_Black','race_Hisp','race_Asian','pct_Black','pct_Hisp','pct_Asian','sex_Male','log_income']\n",
    "x = train_df[fields].values\n",
    "y = train_df[['loan_denied']].values\n",
    "lm = LogisticRegression(C=1e9, solver='lbfgs', max_iter=4000) \n",
    "lm.fit(x,y)\n",
    "\n",
    "results = pd.DataFrame(np.exp(lm.coef_), columns=fields)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting your coefficients and odds ratios\n",
    "\n",
    "After you run your regression **using sklearn**, you can use code like the below to print out an ordered list of features, coefficients, and odds ratios.\n",
    "\n",
    "```python\n",
    "feature_names = X.columns\n",
    "coefficients = clf.coef_[0]\n",
    "\n",
    "pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient (log odds ratio)': coefficients,\n",
    "    'odds ratio': np.exp(coefficients)\n",
    "}).sort_values(by='odds ratio', ascending=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-f10902d08af4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcoefficients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m pd.DataFrame({\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'feature'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "feature_names = X.columns\n",
    "coefficients = clf.coef_[0]\n",
    "\n",
    "pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient (log odds ratio)': coefficients,\n",
    "    'odds ratio': np.exp(coefficients)\n",
    "}).sort_values(by='odds ratio', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait, what's the odds ratio again?\n",
    "\n",
    "It's how much that variable affects the outcome **if all other variables stay the same.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting and thinking about the analysis\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Our results aren't exactly the same as Reveal's, as I pulled a slightly different number of rows from the database and I'm not sure what exact dataset they used for census information. How are we feeling about this reproduction? **You might want check their 2015 results in the whitepaper.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As different as it may be, I believe it shows a certain tendency. I keep reafirming that journalism needs to ba eas \n",
    "#transparent as possible. In their place, I would have made available the exact datasets they used in thar period of time\n",
    "#and disclosure their steps to give as much context to the analysis as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "In the opening paragraph to the flagship piece, [Aaron and Emmanuel write](https://www.revealnews.org/article/for-people-of-color-banks-are-shutting-the-door-to-homeownership/):\n",
    "\n",
    "> Fifty years after the federal Fair Housing Act banned racial discrimination in lending, African Americans and Latinos continue to be routinely denied conventional mortgage loans at rates far higher than their white counterparts.\n",
    "\n",
    "If you look at the results, Hawaiians/Pacific Islanders (and maybe Native Americans) have an even higher odds ratio. **Why do they choose to talk about African Americans and Latinos instead?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JUst a guess, it could be just a lack of atention or historical context (?) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Write a sentence expressing the meaning of the **odds ratio** statistic for Black mortgage applicants. Find a line in [the Reveal piece](https://www.revealnews.org/article/for-people-of-color-banks-are-shutting-the-door-to-homeownership/) where they use the odds ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"[...] where Reveal found African Americans were 2.7 times as likely as whites to be denied a conventional mortgage.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Write a similar sentence about men."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Why did Aaron and Emmanuel choose to include the loan-to-income ratio statistic? **You might want to read the whitepaper.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Credit score is a common reason why loans are denied. Why are credit scores not included in our analysis? **You might want to read the whitepaper.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "This data was just sitting out there for anyone to look at, they didn't even need to FOIA it. Why do you think this issue had not come up before Reveal's analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "As a result of this series, [a lot has happened](https://www.revealnews.org/blog/we-exposed-modern-day-redlining-in-61-cities-find-out-whats-happened-since/), although [recent changes don't look so good](https://www.revealnews.org/blog/cfpb-moves-to-limit-home-loan-data/). If you were reporting this story, what groups of people would you want to talk to in order to make sure you're getting the story right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "When they were consulting experts, Aaron and Emmanuel received a lot of conflicting accounts about whether they should include the \"N/A\" values for race (they ended up including it). If the experts disagreed about something like that, why do you think they went forward with their analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "What if we were working on this story, and our logistic regression or input dataset were flawed? What would be the repercussions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
